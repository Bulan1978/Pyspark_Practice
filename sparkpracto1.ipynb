{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO7+8TZAmu2OF7LWV9K53Qx"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1xfDQm5N93Io","executionInfo":{"status":"ok","timestamp":1710067661506,"user_tz":-330,"elapsed":39134,"user":{"displayName":"Subrata Kar","userId":"16595895468424060358"}},"outputId":"c8f644b4-4de3-4475-e7cd-60a76020b3c5"},"outputs":[{"output_type":"stream","name":"stdout","text":["\r0% [Working]\r            \rHit:1 http://archive.ubuntu.com/ubuntu jammy InRelease\n","\r0% [Waiting for headers] [Waiting for headers] [Waiting for headers] [Connecting to ppa.launchpadcon\r                                                                                                    \rGet:2 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n","\r                                                                                                    \rGet:3 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n","Hit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n","Get:5 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n","Hit:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n","Hit:7 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n","Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n","Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n","Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [1,843 kB]\n","Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n","Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [1,994 kB]\n","Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,349 kB]\n","Get:14 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [1,951 kB]\n","Get:15 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,076 kB]\n","Get:16 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [1,561 kB]\n","Fetched 10.0 MB in 2s (4,850 kB/s)\n","Reading package lists... Done\n"]}],"source":["!apt-get update # Update apt-get repository.\n","!apt-get install openjdk-8-jdk-headless -qq > /dev/null # Install Java.\n","!wget -q http://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz # Download Apache Sparks.\n","!tar xf spark-3.1.1-bin-hadoop3.2.tgz # Unzip the tgz file.\n","!pip install -q findspark # Install findspark. Adds PySpark to the System path during runtime."]},{"cell_type":"code","source":["import os\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n","os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop3.2\"\n","\n","# Initialize findspark\n","import findspark\n","findspark.init()\n","\n","# Create a PySpark session\n","from pyspark.sql import SparkSession\n","#spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n","spark = SparkSession.builder.appName(\"Read and Write Data Using PySpark\").getOrCreate()\n","spark"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":219},"id":"hj0mjN67-EQJ","executionInfo":{"status":"ok","timestamp":1710069431670,"user_tz":-330,"elapsed":13,"user":{"displayName":"Subrata Kar","userId":"16595895468424060358"}},"outputId":"a641441d-0257-449f-c019-4eca02988e9e"},"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<pyspark.sql.session.SparkSession at 0x7c86f9a01000>"],"text/html":["\n","            <div>\n","                <p><b>SparkSession - in-memory</b></p>\n","                \n","        <div>\n","            <p><b>SparkContext</b></p>\n","\n","            <p><a href=\"http://4c1c6d52192e:4040\">Spark UI</a></p>\n","\n","            <dl>\n","              <dt>Version</dt>\n","                <dd><code>v3.1.1</code></dd>\n","              <dt>Master</dt>\n","                <dd><code>local[*]</code></dd>\n","              <dt>AppName</dt>\n","                <dd><code>pyspark-shell</code></dd>\n","            </dl>\n","        </div>\n","        \n","            </div>\n","        "]},"metadata":{},"execution_count":16}]},{"cell_type":"code","source":["# How to convert the index of a PySpark DataFrame into a column?\n","from pyspark.sql.window import Window\n","from pyspark.sql.functions import row_number, monotonically_increasing_id\n","\n","df = spark.createDataFrame([\n","(\"Alice\", 1),\n","(\"Bob\", 2),\n","(\"Charlie\", 3),\n","], [\"Name\", \"Value\"])\n","\n","# Define window specification\n","w = Window.orderBy(monotonically_increasing_id())\n","# Add index\n","df = df.withColumn(\"index\", row_number().over(w) - 1)\n","df.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N8IZjDnl-yh3","executionInfo":{"status":"ok","timestamp":1710067995889,"user_tz":-330,"elapsed":9911,"user":{"displayName":"Subrata Kar","userId":"16595895468424060358"}},"outputId":"40905378-8567-47ff-9b77-5386d2b3b73c"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["+-------+-----+-----+\n","|   Name|Value|index|\n","+-------+-----+-----+\n","|  Alice|    1|    0|\n","|    Bob|    2|    1|\n","|Charlie|    3|    2|\n","+-------+-----+-----+\n","\n"]}]},{"cell_type":"markdown","source":["Reading DataFrames, Reading and writing files"],"metadata":{"id":"q3yYc9feAKcv"}},{"cell_type":"code","source":["data = [(\"Alice\", 34), (\"Bob\", 45), (\"Cathy\", 29)]\n","columns = [\"Name\", \"Age\"]\n","df = spark.createDataFrame(data, columns)\n","df.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0k5ggrhyANhn","executionInfo":{"status":"ok","timestamp":1710068302038,"user_tz":-330,"elapsed":728,"user":{"displayName":"Subrata Kar","userId":"16595895468424060358"}},"outputId":"572021ca-a6f7-4d0e-850b-17390256b508"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["+-----+---+\n","| Name|Age|\n","+-----+---+\n","|Alice| 34|\n","|  Bob| 45|\n","|Cathy| 29|\n","+-----+---+\n","\n"]}]},{"cell_type":"code","source":["#--reading a CSV file\n","csv_file = \"/content/sample_data/california_housing_train.csv\"\n","df_csv = spark.read.csv(csv_file, header=True, inferSchema=True)\n","#--writing to a CSV file\n","df_csv.write.csv(\"chtw.csv\", header = True, mode = \"overwrite\")"],"metadata":{"id":"Eq5sYTdkBFei","executionInfo":{"status":"ok","timestamp":1710068442242,"user_tz":-330,"elapsed":2350,"user":{"displayName":"Subrata Kar","userId":"16595895468424060358"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["Reading and writing to json files"],"metadata":{"id":"tnKSu6hHCV8B"}},{"cell_type":"code","source":["json_file = \"/content/sample_data/anscombe.json\"\n","#--reading\n","df_json = spark.read.json(json_file)\n","#--writing\n","df_json.write.json(\"a.json\", mode = \"overwrite\")"],"metadata":{"id":"hQX6PDAgBbvx","executionInfo":{"status":"ok","timestamp":1710068930100,"user_tz":-330,"elapsed":1418,"user":{"displayName":"Subrata Kar","userId":"16595895468424060358"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["Creating an SQL table in pyspark"],"metadata":{"id":"mdEalsnsDeS2"}},{"cell_type":"code","source":["data = [\n","    {\"name\": \"Alice\", \"age\": 30, \"city\": \"New York\"},\n","    {\"name\": \"Bob\", \"age\": 25, \"city\": \"San Francisco\"},\n","    {\"name\": \"Charlie\", \"age\": 35, \"city\": \"Los Angeles\"}\n","]\n","df_sql = spark.createDataFrame(data)"],"metadata":{"id":"VeQdiWNXDixl","executionInfo":{"status":"ok","timestamp":1710069127563,"user_tz":-330,"elapsed":502,"user":{"displayName":"Subrata Kar","userId":"16595895468424060358"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["df_sql.createOrReplaceTempView(\"people\")\n","qry = \"select * from people where age >= 25\"\n","odf = spark.sql(qry)\n","odf.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6xdEbsSkECwf","executionInfo":{"status":"ok","timestamp":1710069265320,"user_tz":-330,"elapsed":1231,"user":{"displayName":"Subrata Kar","userId":"16595895468424060358"}},"outputId":"a28fc2ca-7c73-4e4d-f5ff-594008cf34e2"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["+---+-------------+-------+\n","|age|         city|   name|\n","+---+-------------+-------+\n","| 30|     New York|  Alice|\n","| 25|San Francisco|    Bob|\n","| 35|  Los Angeles|Charlie|\n","+---+-------------+-------+\n","\n"]}]},{"cell_type":"markdown","source":["Converting pandas DataFrame to pyspark DataFrame"],"metadata":{"id":"DNPh_NZtEsV3"}},{"cell_type":"code","source":["import pandas as pd\n","data = [\n","    {\"name\": \"Alice\", \"age\": 30, \"city\": \"New York\"},\n","    {\"name\": \"Bob\", \"age\": 25, \"city\": \"San Francisco\"},\n","    {\"name\": \"Charlie\", \"age\": 35, \"city\": \"Los Angeles\"}\n","]\n","pandasDF = pd.DataFrame(data, columns = ['name', 'age', 'city'])\n","print(pandasDF)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tLLtjYtvEr0V","executionInfo":{"status":"ok","timestamp":1710069623271,"user_tz":-330,"elapsed":479,"user":{"displayName":"Subrata Kar","userId":"16595895468424060358"}},"outputId":"efa4edc4-1cfe-4571-b1e6-8028a73a51e2"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["      name  age           city\n","0    Alice   30       New York\n","1      Bob   25  San Francisco\n","2  Charlie   35    Los Angeles\n"]}]},{"cell_type":"code","source":["sparkDF = spark.createDataFrame(pandasDF)\n","sparkDF.printSchema()\n","sparkDF.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QClfvfXEF8KC","executionInfo":{"status":"ok","timestamp":1710069734206,"user_tz":-330,"elapsed":756,"user":{"displayName":"Subrata Kar","userId":"16595895468424060358"}},"outputId":"34d85db1-ba20-4109-a733-5f3cfe514727"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stderr","text":["/content/spark-3.1.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:331: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n","  for column, series in pdf.iteritems():\n"]},{"output_type":"stream","name":"stdout","text":["root\n"," |-- name: string (nullable = true)\n"," |-- age: long (nullable = true)\n"," |-- city: string (nullable = true)\n","\n","+-------+---+-------------+\n","|   name|age|         city|\n","+-------+---+-------------+\n","|  Alice| 30|     New York|\n","|    Bob| 25|San Francisco|\n","|Charlie| 35|  Los Angeles|\n","+-------+---+-------------+\n","\n"]}]}]}