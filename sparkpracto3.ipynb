{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"wHeB43FIqjtu"},"outputs":[{"name":"stdout","output_type":"stream","text":["Hit:1 http://archive.ubuntu.com/ubuntu jammy InRelease\n","Get:2 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n","Get:3 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n","Get:4 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [109 kB]\n","Get:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n","Get:6 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n","Hit:7 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n","Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n","Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n","Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,351 kB]\n","Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n","Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [1,847 kB]\n","Get:13 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [33.3 kB]\n","Get:14 http://archive.ubuntu.com/ubuntu jammy-backports/main amd64 Packages [80.9 kB]\n","Get:15 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [736 kB]\n","Get:16 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [1,960 kB]\n","Get:17 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,078 kB]\n","Get:18 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [1,568 kB]\n","Fetched 8,998 kB in 2s (4,804 kB/s)\n","Reading package lists... Done\n"]}],"source":["!apt-get update # Update apt-get repository.\n","!apt-get install openjdk-8-jdk-headless -qq \u003e /dev/null # Install Java.\n","!wget -q http://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz # Download Apache Sparks.\n","!tar xf spark-3.1.1-bin-hadoop3.2.tgz # Unzip the tgz file.\n","!pip install -q findspark # Install findspark. Adds PySpark to the System path during runtime."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"oUrToDbnqsw4"},"outputs":[],"source":["import os\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n","os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop3.2\"\n","# Initialize findspark\n","import findspark\n","findspark.init()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"-GReaPoooUtO"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["\"\\nfrom google.colab import drive\\ndrive.mount('/content/drive')\\n\""]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["'''\n","from google.colab import drive\n","drive.mount('/content/drive')\n","'''"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"7jyUgPFvszA9"},"outputs":[{"name":"stdout","output_type":"stream","text":["+---+-------+----+-----+\n","| ID|   NAME|DEPT|  FEE|\n","+---+-------+----+-----+\n","|  1| sravan|  IT|45000|\n","|  2| ojaswi|  CS|85000|\n","|  3| rohith|  CS|41000|\n","|  4|sridevi|  IT|56000|\n","|  5|  bobby| ECE|45000|\n","|  6|gayatri| ECE|49000|\n","|  7|gnanesh|  CS|45000|\n","|  8|  bhanu|Mech|21000|\n","+---+-------+----+-----+\n","\n"]}],"source":["from pyspark.sql import SparkSession\n","spark = SparkSession.builder.appName('sparkdf').getOrCreate()\n","\n","data = [[\"1\", \"sravan\", \"IT\", 45000],\n","        [\"2\", \"ojaswi\", \"CS\", 85000],\n","        [\"3\", \"rohith\", \"CS\", 41000],\n","        [\"4\", \"sridevi\", \"IT\", 56000],\n","        [\"5\", \"bobby\", \"ECE\", 45000],\n","        [\"6\", \"gayatri\", \"ECE\", 49000],\n","        [\"7\", \"gnanesh\", \"CS\", 45000],\n","        [\"8\", \"bhanu\", \"Mech\", 21000]]\n","\n","columns = ['ID', 'NAME', 'DEPT', 'FEE']\n","df = spark.createDataFrame(data, columns)\n","\n","df.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"50MD3_KctogL"},"outputs":[{"name":"stdout","output_type":"stream","text":["+----+--------------------+---------------------+\n","|DEPT|Dept wise total fees|Average fees per dept|\n","+----+--------------------+---------------------+\n","|ECE |94000               |47000.0              |\n","|IT  |101000              |50500.0              |\n","|CS  |171000              |57000.0              |\n","|Mech|21000               |21000.0              |\n","+----+--------------------+---------------------+\n","\n"]}],"source":["from pyspark.sql import functions as f\n","gcols = ['DEPT']\n","\n","df.groupBy(gcols).agg(\n","    f.sum('fee').alias(\"Dept wise total fees\"),\n","    f.avg('fee').alias(\"Average fees per dept\")\n",").show(truncate=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"gY6PmhqKxb5J"},"outputs":[{"name":"stdout","output_type":"stream","text":["+----+--------------------+---------------------+\n","|dept|Dept_wise_total_fees|Average_fees_per_dept|\n","+----+--------------------+---------------------+\n","|  CS|              171000|              57000.0|\n","| ECE|               94000|              47000.0|\n","|  IT|              101000|              50500.0|\n","|Mech|               21000|              21000.0|\n","+----+--------------------+---------------------+\n","\n"]}],"source":["df.createOrReplaceTempView(\"students\")\n","sql_str = \"select dept, sum(fee) as Dept_wise_total_fees, avg(fee) as Average_fees_per_dept\"\\\n","\" from students\"\\\n","\" group by dept\" \\\n","\" order by dept\"\n","\n","spark.sql(sql_str).show()"]},{"cell_type":"markdown","metadata":{"id":"fU7mctst0xcu"},"source":["Inner join using join function"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"--wDXg2b0zBT"},"outputs":[{"name":"stdout","output_type":"stream","text":["df1 show\n","+---+-------+---------+\n","| ID|   NAME|  Company|\n","+---+-------+---------+\n","|  1| sravan|company 1|\n","|  2| ojaswi|company 1|\n","|  3| rohith|company 2|\n","|  4|sridevi|company 1|\n","|  5|  bobby|company 1|\n","+---+-------+---------+\n","\n","df2 show\n","+---+------+----------+\n","| ID|salary|department|\n","+---+------+----------+\n","|  1| 45000|        IT|\n","|  2|145000|   Manager|\n","|  6| 45000|        HR|\n","|  5| 34000|     Sales|\n","+---+------+----------+\n","\n"]}],"source":["data = [[\"1\", \"sravan\", \"company 1\"],\n","        [\"2\", \"ojaswi\", \"company 1\"],\n","        [\"3\", \"rohith\", \"company 2\"],\n","        [\"4\", \"sridevi\", \"company 1\"],\n","        [\"5\", \"bobby\", \"company 1\"]]\n","\n","# specify column names\n","columns = ['ID', 'NAME', 'Company']\n","\n","df1 = spark.createDataFrame(data, columns)\n","print (\"df1 show\")\n","df1.show()\n","\n","data = [[\"1\", \"45000\", \"IT\"],\n","         [\"2\", \"145000\", \"Manager\"],\n","         [\"6\", \"45000\", \"HR\"],\n","         [\"5\", \"34000\", \"Sales\"]]\n","\n","# specify column names\n","columns = ['ID', 'salary', 'department']\n","\n","df2 = spark.createDataFrame(data, columns)\n","print (\"df2 show\")\n","df2.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"W7BXjuO-1WA_"},"outputs":[{"name":"stdout","output_type":"stream","text":["+---+------+---------+---+------+----------+\n","| ID|  NAME|  Company| ID|salary|department|\n","+---+------+---------+---+------+----------+\n","|  5| bobby|company 1|  5| 34000|     Sales|\n","|  1|sravan|company 1|  1| 45000|        IT|\n","|  2|ojaswi|company 1|  2|145000|   Manager|\n","+---+------+---------+---+------+----------+\n","\n"]}],"source":["#--join functions\n","df1.join(df2, df1.ID == df2.ID, \"inner\").show()"]},{"cell_type":"markdown","metadata":{"id":"TJ-1IZuL158G"},"source":["Inner join using sql"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"fCfqaYPd17uW"},"outputs":[{"name":"stdout","output_type":"stream","text":["+---+------+---------+---+------+----------+\n","| ID|  NAME|  Company| ID|salary|department|\n","+---+------+---------+---+------+----------+\n","|  5| bobby|company 1|  5| 34000|     Sales|\n","|  1|sravan|company 1|  1| 45000|        IT|\n","|  2|ojaswi|company 1|  2|145000|   Manager|\n","+---+------+---------+---+------+----------+\n","\n"]}],"source":["df1.createOrReplaceTempView(\"emp\")\n","df2.createOrReplaceTempView(\"dept\")\n","sql_str = \"select * from emp e, dept d where e.ID = d.ID\"\n","spark.sql(sql_str).show()"]},{"cell_type":"markdown","metadata":{"id":"H_N_qycK21Tq"},"source":["Union using function and sql query"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"2RQMVTN8248e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Union using union function\n","+------------+------------------+\n","|Student Name|Overall Percentage|\n","+------------+------------------+\n","|   Bhuwanesh|             82.98|\n","|     Harshit|             80.31|\n","|      Naveen|            91.123|\n","|      Piyush|             90.51|\n","+------------+------------------+\n","\n","Union using SQL query\n","+------------+------------------+\n","|Student Name|Overall Percentage|\n","+------------+------------------+\n","|      Naveen|            91.123|\n","|   Bhuwanesh|             82.98|\n","|      Piyush|             90.51|\n","|     Harshit|             80.31|\n","+------------+------------------+\n","\n"]}],"source":["#--create dataframes\n","df1 = spark.createDataFrame(\n","    [(\"Bhuwanesh\", 82.98), (\"Harshit\", 80.31)],\n","    [\"Student Name\", \"Overall Percentage\"]\n","    )\n","\n","df2 = spark.createDataFrame(\n","    [(\"Naveen\", 91.123), (\"Piyush\", 90.51)],\n","    [\"Student Name\", \"Overall Percentage\"]\n",")\n","\n","print (\"Union using union function\")\n","df1.union(df2).show()\n","\n","print (\"Union using SQL query\")\n","df1.createOrReplaceTempView(\"set1\")\n","df2.createOrReplaceTempView(\"set2\")\n","\n","sql_qry = \"select * from set1\"\\\n","\" union\"\\\n","\" select * from set2\"\n","spark.sql(sql_qry).show()\n"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPFhrCbALMjuWo0t9MI3saZ","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}